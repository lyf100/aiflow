# AIFlow ç”¨æˆ·ä½¿ç”¨æŒ‡å—

**User Guide for AIFlow Code Behavior Sandbox**

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [å®Œæ•´å·¥ä½œæµç¨‹](#å®Œæ•´å·¥ä½œæµç¨‹)
3. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
4. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ææ¡ä»¶

- **Python 3.11+**
- **Node.js 18+**
- **AI API Key**ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰ï¼š
  - Anthropic API Key (Claude 4.5)
  - OpenAI API Key (GPT-4)
  - Google API Key (Gemini)

### 5åˆ†é’Ÿå¿«é€Ÿä½“éªŒ

#### Step 1: å®‰è£…ä¾èµ–

```bash
# åç«¯
cd backend
pip install -r requirements.txt

# å‰ç«¯
cd ../frontend
npm install
```

#### Step 2: é…ç½® API Key

åˆ›å»º `backend/.env` æ–‡ä»¶ï¼š

```bash
# é€‰æ‹©ä¸€ä¸ª AI æœåŠ¡ï¼ˆæ¨è Claude 4.5ï¼‰
ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# å¯é€‰ï¼šå…¶ä»– AI æœåŠ¡
# OPENAI_API_KEY=sk-your-openai-key-here
# GOOGLE_API_KEY=your-google-key-here
```

#### Step 3: åˆ†æä½ çš„ç¬¬ä¸€ä¸ªé¡¹ç›®

```python
# demo.py
import asyncio
import json
from pathlib import Path
from aiflow.prompts import PromptRenderer
from aiflow.adapters.claude import create_claude_adapter

async def analyze_project():
    # 1. å‡†å¤‡è¾“å…¥æ•°æ®
    project_path = Path("path/to/your/project")

    # 2. æ¸²æŸ“ Prompt æ¨¡æ¿
    renderer = PromptRenderer()
    prompt = renderer.render(
        language="python",
        stage="comprehensive_analysis",
        input_data={
            "project_name": "MyApp",
            "project_path": str(project_path),
            "file_tree": "..."  # ä½ çš„é¡¹ç›®æ–‡ä»¶æ ‘
        }
    )

    # 3. é€šè¿‡ AI é€‚é…å™¨æ‰§è¡Œåˆ†æ
    adapter = await create_claude_adapter()
    response = await adapter.generate(prompt=prompt)

    # 4. ä¿å­˜ç»“æœ
    output_file = Path("analysis_result.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(response.content, f, indent=2, ensure_ascii=False)

    print(f"âœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

# è¿è¡Œ
asyncio.run(analyze_project())
```

#### Step 4: å¯åŠ¨å¯è§†åŒ–å‰ç«¯

```bash
cd frontend
npm run dev
```

æ‰“å¼€æµè§ˆå™¨è®¿é—® `http://localhost:5173`ï¼Œæ‹–æ‹½ `analysis_result.json` æ–‡ä»¶åˆ°é¡µé¢ä¸­ã€‚

---

## ğŸ“Š å®Œæ•´å·¥ä½œæµç¨‹

### æµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·å‡†å¤‡ â†’ AIåˆ†æ â†’ æ•°æ®éªŒè¯ â†’ å¯è§†åŒ–æ¸²æŸ“ â†’ äº¤äº’æ¢ç´¢
```

### è¯¦ç»†æ­¥éª¤

#### é˜¶æ®µ1: å‡†å¤‡é¡¹ç›®æ•°æ®

**ç›®æ ‡**ï¼šæ”¶é›†é¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯å’Œæ–‡ä»¶ç»“æ„ã€‚

**æ“ä½œ**ï¼š

```python
from pathlib import Path

def collect_project_info(project_path: Path):
    """æ”¶é›†é¡¹ç›®ä¿¡æ¯"""
    info = {
        "project_name": project_path.name,
        "project_path": str(project_path),
        "file_tree": generate_file_tree(project_path),
        "main_files": []
    }

    # æ”¶é›†å…³é”®æ–‡ä»¶
    key_files = [
        "README.md",
        "package.json",
        "requirements.txt",
        "pyproject.toml"
    ]

    for file_name in key_files:
        file_path = project_path / file_name
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                info["main_files"].append({
                    "name": file_name,
                    "content": f.read()
                })

    return info

def generate_file_tree(path: Path, prefix="", max_depth=3, current_depth=0):
    """ç”Ÿæˆæ–‡ä»¶æ ‘å­—ç¬¦ä¸²"""
    if current_depth >= max_depth:
        return ""

    tree = ""
    items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))

    for i, item in enumerate(items):
        is_last = i == len(items) - 1
        connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
        tree += f"{prefix}{connector}{item.name}\n"

        if item.is_dir() and not item.name.startswith('.'):
            extension = "    " if is_last else "â”‚   "
            tree += generate_file_tree(item, prefix + extension, max_depth, current_depth + 1)

    return tree
```

#### é˜¶æ®µ2: æ¸²æŸ“ Prompt æ¨¡æ¿

**ç›®æ ‡**ï¼šæ ¹æ®é¡¹ç›®è¯­è¨€é€‰æ‹©åˆé€‚çš„ Prompt æ¨¡æ¿ã€‚

**æ“ä½œ**ï¼š

```python
from aiflow.prompts import PromptRenderer

renderer = PromptRenderer()

# æ–¹å¼ 1: ä½¿ç”¨ç»¼åˆåˆ†ææ¨¡æ¿ï¼ˆæ¨èï¼‰
prompt = renderer.render(
    language="python",
    stage="comprehensive_analysis",
    input_data=project_info
)

# æ–¹å¼ 2: åˆ†é˜¶æ®µåˆ†æ
prompts = []
for stage in ["project_understanding", "structure_recognition",
              "semantic_analysis", "execution_inference",
              "concurrency_detection"]:
    prompt = renderer.render(
        language="python",
        stage=stage,
        input_data=project_info
    )
    prompts.append(prompt)
```

**é€‰æ‹©ç­–ç•¥**ï¼š
- **å°å‹é¡¹ç›®**ï¼ˆ<1000è¡Œï¼‰ï¼šä½¿ç”¨ç»¼åˆåˆ†ææ¨¡æ¿ï¼Œä¸€æ¬¡å®Œæˆ
- **ä¸­å‹é¡¹ç›®**ï¼ˆ1000-5000è¡Œï¼‰ï¼šä½¿ç”¨ç»¼åˆåˆ†ææ¨¡æ¿æˆ–åˆ†é˜¶æ®µ
- **å¤§å‹é¡¹ç›®**ï¼ˆ>5000è¡Œï¼‰ï¼šå»ºè®®åˆ†é˜¶æ®µåˆ†æï¼Œæ¯é˜¶æ®µç»“æœä½œä¸ºä¸‹ä¸€é˜¶æ®µè¾“å…¥

#### é˜¶æ®µ3: é€šè¿‡ AI é€‚é…å™¨æ‰§è¡Œåˆ†æ

**ç›®æ ‡**ï¼šè°ƒç”¨ AI æœåŠ¡å®Œæˆä»£ç åˆ†æã€‚

**æ“ä½œ**ï¼š

```python
from aiflow.adapters.claude import create_claude_adapter

# åˆ›å»ºé€‚é…å™¨
adapter = await create_claude_adapter(
    model_name="claude-sonnet-4-5-20250929",  # Claude 4.5
    max_tokens=8192,
    temperature=0.3  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
)

# æ‰§è¡Œåˆ†æ
try:
    response = await adapter.generate(
        prompt=prompt,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œç²¾é€šå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚"
    )

    print(f"âœ… åˆ†ææˆåŠŸï¼")
    print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage.input_tokens + response.usage.output_tokens}")
    print(f"â±ï¸ è€—æ—¶: {response.metadata.get('duration', 'N/A')}ç§’")

    result = response.content

except Exception as e:
    print(f"âŒ åˆ†æå¤±è´¥: {e}")
    # é”™è¯¯å¤„ç†é€»è¾‘
```

**æµå¼è¾“å‡º**ï¼ˆé€‚ç”¨äºå¤§å‹é¡¹ç›®ï¼‰ï¼š

```python
async for chunk in adapter.generate_stream(prompt=prompt):
    print(chunk, end='', flush=True)
```

#### é˜¶æ®µ4: éªŒè¯åˆ†æç»“æœ

**ç›®æ ‡**ï¼šç¡®ä¿ AI è¾“å‡ºç¬¦åˆæ ‡å‡†æ•°æ®åè®®ã€‚

**æ“ä½œ**ï¼š

```python
from aiflow.protocol import ProtocolValidator

validator = ProtocolValidator()

# å®Œæ•´éªŒè¯
validation_result = validator.validate_complete(result)

if validation_result.is_valid:
    print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
else:
    print("âŒ æ•°æ®éªŒè¯å¤±è´¥:")
    for error in validation_result.errors:
        print(f"  - {error}")

    # å°è¯•è‡ªåŠ¨ä¿®å¤
    if validation_result.warnings:
        print("\nâš ï¸ è­¦å‘Šï¼ˆå·²è‡ªåŠ¨ä¿®å¤ï¼‰:")
        for warning in validation_result.warnings:
            print(f"  - {warning}")
```

**éªŒè¯å†…å®¹**ï¼š
- âœ… JSON Schema æ ¼å¼éªŒè¯
- âœ… UUID v4 æ ¼å¼éªŒè¯
- âœ… ISO 8601 æ—¶é—´æˆ³éªŒè¯
- âœ… å¼•ç”¨å®Œæ•´æ€§éªŒè¯ï¼ˆ10ç§å¼•ç”¨ç±»å‹ï¼‰
- âœ… æ‰§è¡Œé¡ºåºå”¯ä¸€æ€§éªŒè¯

#### é˜¶æ®µ5: åºåˆ—åŒ–å’Œä¿å­˜

**ç›®æ ‡**ï¼šå°†åˆ†æç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚

**æ“ä½œ**ï¼š

```python
from aiflow.protocol import ProtocolSerializer

serializer = ProtocolSerializer()

# æ–¹å¼ 1: ä¿å­˜ä¸ºæ ‡å‡† JSON
serializer.serialize(
    data=result,
    output_path="analysis_result.json",
    compress=False,
    validate=True
)

# æ–¹å¼ 2: ä¿å­˜ä¸ºå‹ç¼© JSONï¼ˆèŠ‚çœ 70% ç©ºé—´ï¼‰
serializer.serialize(
    data=result,
    output_path="analysis_result.json",
    compress=True,  # ç”Ÿæˆ .json.gz æ–‡ä»¶
    validate=True
)

# æ–¹å¼ 3: å¢é‡æ›´æ–°ï¼ˆé€‚ç”¨äºåˆ†é˜¶æ®µåˆ†æï¼‰
serializer.update_partial(
    file_path="analysis_result.json",
    updates={
        "execution_trace": stage4_result,
        "concurrency_info": stage5_result
    },
    validate=True
)
```

#### é˜¶æ®µ6: å¯åŠ¨å¯è§†åŒ–å‰ç«¯

**ç›®æ ‡**ï¼šåœ¨æµè§ˆå™¨ä¸­æ¸²æŸ“äº¤äº’å¼è¡Œä¸ºæ²™ç›˜ã€‚

**æ“ä½œ**ï¼š

```bash
cd frontend
npm run dev
```

**åŠ è½½æ•°æ®**ï¼š
1. æ‰“å¼€æµè§ˆå™¨è®¿é—® `http://localhost:5173`
2. æ‹–æ‹½ `analysis_result.json` åˆ°é¡µé¢ä¸­
3. æˆ–ç‚¹å‡»"åŠ è½½æ–‡ä»¶"æŒ‰é’®é€‰æ‹©æ–‡ä»¶

#### é˜¶æ®µ7: äº¤äº’å¼æ¢ç´¢

**ç›®æ ‡**ï¼šé€šè¿‡å¯è§†åŒ–ç•Œé¢ç†è§£ä»£ç è¡Œä¸ºã€‚

**æ ¸å¿ƒäº¤äº’**ï¼š

**1. æµè§ˆä»£ç ç»“æ„**
```
æ“ä½œ:
  - é¼ æ ‡æ»šè½®: ç¼©æ”¾
  - é¼ æ ‡æ‹–æ‹½: å¹³ç§»
  - ç‚¹å‡»èŠ‚ç‚¹: æŸ¥çœ‹è¯¦æƒ…
  - åŒå‡»èŠ‚ç‚¹: å±•å¼€/æŠ˜å å­èŠ‚ç‚¹
```

**2. å¯åŠ¨æŒ‰é’®äº¤äº’**
```
å¤§æŒ‰é’®ï¼ˆMacroï¼‰:
  - ç‚¹å‡»: å¯åŠ¨ç³»ç»Ÿçº§å¤šæµç¨‹è”åŠ¨åŠ¨ç”»
  - è§‚å¯Ÿ: å¤šæ¡"è„‰å†²"åŒæ—¶åœ¨æ²™ç›˜ä¸ŠæµåŠ¨

å°æŒ‰é’®ï¼ˆMicroï¼‰:
  - ç‚¹å‡»: å¯åŠ¨å­æ¨¡å—ç‹¬ç«‹æµç¨‹åŠ¨ç”»
  - è§‚å¯Ÿ: å•ä¸€æµç¨‹çš„æ‰§è¡Œè·¯å¾„
```

**3. æ‰§è¡Œè½¨è¿¹åˆ‡æ¢**
```
è§†å›¾åˆ‡æ¢:
  - ç»“æ„å›¾ (Cytoscape): ä»£ç å±‚çº§å’Œä¾èµ–å…³ç³»
  - æ—¶åºå›¾ (D3): ç»„ä»¶é—´çš„æ¶ˆæ¯ä¼ é€’
  - æµç¨‹å›¾ (D3): æ‰§è¡Œæ­¥éª¤å’Œåˆ†æ”¯å†³ç­–
  - å•æ­¥è¯¦æƒ… (Monaco): ä»£ç  + å˜é‡ + è°ƒç”¨æ ˆ
```

**4. å•æ­¥è°ƒè¯•**
```
æ“ä½œ:
  - â®ï¸ ä¸Šä¸€æ­¥: å›åˆ°å‰ä¸€ä¸ªæ‰§è¡Œæ­¥éª¤
  - â¯ï¸ æ’­æ”¾/æš‚åœ: æ§åˆ¶åŠ¨ç”»æ’­æ”¾
  - â­ï¸ ä¸‹ä¸€æ­¥: å‰è¿›åˆ°ä¸‹ä¸€ä¸ªæ‰§è¡Œæ­¥éª¤
  - ğŸ” æŸ¥çœ‹å˜é‡: æ£€æŸ¥å½“å‰ä½œç”¨åŸŸçš„å˜é‡çŠ¶æ€
  - ğŸ“š æŸ¥çœ‹å †æ ˆ: æŸ¥çœ‹å‡½æ•°è°ƒç”¨é“¾
```

**5. å¹¶å‘æµç¨‹è§‚å¯Ÿ**
```
å¤šæµç¨‹è”åŠ¨:
  - è“è‰²è„‰å†²: ä¸»çº¿ç¨‹æ‰§è¡Œè·¯å¾„
  - ç»¿è‰²è„‰å†²: å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œè·¯å¾„
  - çº¢è‰²è„‰å†²: é”™è¯¯å¤„ç†è·¯å¾„
  - é»„è‰²èŠ‚ç‚¹: åŒæ­¥ç‚¹ï¼ˆé”ã€ä¿¡å·é‡ç­‰ï¼‰
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰ Prompt æ¨¡æ¿

**åœºæ™¯**ï¼šä¸ºç‰¹å®šé¡¹ç›®ä¼˜åŒ–åˆ†ææ•ˆæœã€‚

**æ­¥éª¤**ï¼š

1. å¤åˆ¶ç°æœ‰æ¨¡æ¿
```bash
cp backend/prompts/python/comprehensive/analysis/v1.0.0.yaml \
   backend/prompts/python/comprehensive/analysis/v1.1.0.yaml
```

2. ç¼–è¾‘æ¨¡æ¿
```yaml
metadata:
  id: "comprehensive-python-django-v1.1.0"
  version: "1.1.0"
  description: "ä¸“é—¨é’ˆå¯¹Djangoé¡¹ç›®ä¼˜åŒ–çš„åˆ†ææ¨¡æ¿"

template: |
  # åœ¨è¿™é‡Œæ·»åŠ Djangoç‰¹å®šçš„åˆ†ææŒ‡ä»¤

  ## Djangoé¡¹ç›®ç‰¹æ®Šå¤„ç†
  1. è¯†åˆ« apps/ ç›®å½•ä¸‹çš„å„ä¸ªåº”ç”¨
  2. åˆ†æ models.py ä¸­çš„æ•°æ®æ¨¡å‹
  3. è¯†åˆ« views.py ä¸­çš„è§†å›¾å‡½æ•°
  4. åˆ†æ urls.py ä¸­çš„è·¯ç”±é…ç½®
  ...
```

3. æ³¨å†Œæ¨¡æ¿
```yaml
# backend/prompts/registry.yaml
templates:
  - id: "comprehensive-python-django-v1.1.0"
    path: "python/comprehensive/analysis/v1.1.0.yaml"
    language: "python"
    stage: "comprehensive_analysis"
    version: "1.1.0"
    latest: false
```

4. ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿
```python
prompt = renderer.render(
    language="python",
    stage="comprehensive_analysis",
    version="1.1.0",  # æŒ‡å®šç‰ˆæœ¬
    input_data=project_info
)
```

### 2. å¤šæ¨¡å‹ååŒåˆ†æ

**åœºæ™¯**ï¼šä½¿ç”¨ä¸åŒ AI æ¨¡å‹çš„ä¼˜åŠ¿ã€‚

**ç­–ç•¥**ï¼š

```python
# Claude 4.5: æ“…é•¿æ¨ç†å’Œè¯­ä¹‰ç†è§£
claude_adapter = await create_claude_adapter()

# GPT-4: æ“…é•¿åˆ›é€ æ€§å‘½å
openai_adapter = await create_openai_adapter()

# é˜¶æ®µ 1-2: ä½¿ç”¨ Claudeï¼ˆç»“æ„åˆ†æï¼‰
structure_result = await claude_adapter.generate(
    prompt=structure_prompt
)

# é˜¶æ®µ 3: ä½¿ç”¨ GPT-4ï¼ˆä¸šåŠ¡å‘½åï¼‰
semantic_result = await openai_adapter.generate(
    prompt=semantic_prompt
)

# åˆå¹¶ç»“æœ
final_result = merge_results(structure_result, semantic_result)
```

### 3. æ‰¹é‡é¡¹ç›®åˆ†æ

**åœºæ™¯**ï¼šåˆ†æå¤šä¸ªç›¸å…³é¡¹ç›®ã€‚

**å®ç°**ï¼š

```python
import asyncio
from pathlib import Path

async def batch_analyze(projects: list[Path]):
    """æ‰¹é‡åˆ†æå¤šä¸ªé¡¹ç›®"""
    tasks = []
    for project_path in projects:
        task = analyze_project(project_path)
        tasks.append(task)

    # å¹¶å‘æ‰§è¡Œï¼ˆæ³¨æ„ API é€Ÿç‡é™åˆ¶ï¼‰
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # å¤„ç†ç»“æœ
    for project_path, result in zip(projects, results):
        if isinstance(result, Exception):
            print(f"âŒ {project_path.name}: {result}")
        else:
            print(f"âœ… {project_path.name}: åˆ†æå®Œæˆ")
            # ä¿å­˜ç»“æœ
            save_result(project_path, result)

# ä½¿ç”¨
projects = [
    Path("projects/frontend"),
    Path("projects/backend"),
    Path("projects/mobile")
]
await batch_analyze(projects)
```

### 4. å¢é‡åˆ†æ

**åœºæ™¯**ï¼šä»£ç æ›´æ–°åï¼Œåªé‡æ–°åˆ†æå˜æ›´éƒ¨åˆ†ã€‚

**å®ç°**ï¼š

```python
from aiflow.protocol import ProtocolSerializer

serializer = ProtocolSerializer()

# 1. æ£€æµ‹æ–‡ä»¶å˜æ›´
changed_files = detect_changes(project_path, last_analysis_time)

# 2. åªåˆ†æå˜æ›´çš„æ¨¡å—
partial_result = await analyze_partial(
    changed_files=changed_files,
    previous_result=load_previous_result()
)

# 3. å¢é‡æ›´æ–° JSON
serializer.update_partial(
    file_path="analysis_result.json",
    updates=partial_result,
    validate=True
)
```

### 5. å¯¼å‡ºå’Œåˆ†äº«

**åœºæ™¯**ï¼šå°†åˆ†æç»“æœåˆ†äº«ç»™å›¢é˜Ÿæˆå‘˜ã€‚

**å¯¼å‡ºæ ¼å¼**ï¼š

```python
from aiflow.export import ResultExporter

exporter = ResultExporter()

# å¯¼å‡ºä¸ºé™æ€ HTMLï¼ˆåŒ…å«å¯è§†åŒ–ï¼‰
exporter.to_html(
    result_file="analysis_result.json",
    output_file="analysis_report.html"
)

# å¯¼å‡ºä¸º Markdown æŠ¥å‘Š
exporter.to_markdown(
    result_file="analysis_result.json",
    output_file="analysis_report.md"
)

# å¯¼å‡ºä¸º PDF
exporter.to_pdf(
    result_file="analysis_result.json",
    output_file="analysis_report.pdf"
)
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: AI è¾“å‡ºæ ¼å¼ä¸æ­£ç¡®

**ç—‡çŠ¶**ï¼š
```
âŒ æ•°æ®éªŒè¯å¤±è´¥:
  - èŠ‚ç‚¹ID '1' ä¸æ˜¯æœ‰æ•ˆçš„UUID v4æ ¼å¼
  - æ—¶é—´æˆ³ '2025-01-12 10:30:00' ä¸æ˜¯ISO 8601æ ¼å¼
```

**åŸå› **ï¼šPrompt æŒ‡ä»¤ä¸å¤Ÿæ˜ç¡®ï¼ŒAI æœªä¸¥æ ¼éµå®ˆæ ¼å¼è¦æ±‚ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **å¢å¼º Prompt çº¦æŸ**
```yaml
# åœ¨ Prompt æ¨¡æ¿ä¸­å¼ºè°ƒ
<constraints>
- æ‰€æœ‰IDå¿…é¡»ä¸¥æ ¼ä½¿ç”¨UUID v4æ ¼å¼: xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx
- æ‰€æœ‰æ—¶é—´æˆ³å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ISO 8601æ ¼å¼: 2025-01-12T10:30:00.000Z
- ä¸æ¥å—ä»»ä½•ç®€åŒ–æ ¼å¼
</constraints>
```

2. **æä¾›æ˜ç¡®ç¤ºä¾‹**
```yaml
## è¾“å‡ºç¤ºä¾‹

âœ… æ­£ç¡®:
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "timestamp": "2025-01-12T10:30:00.123Z"
}

âŒ é”™è¯¯:
{
  "id": "1",  // âŒ ä¸æ˜¯UUID
  "timestamp": "2025-01-12 10:30:00"  // âŒ ä¸æ˜¯ISO 8601
}
```

3. **åå¤„ç†ä¿®å¤**
```python
import uuid
from datetime import datetime

def fix_common_issues(data):
    """è‡ªåŠ¨ä¿®å¤å¸¸è§æ ¼å¼é—®é¢˜"""
    # ä¿®å¤IDæ ¼å¼
    if "id" in data and not is_valid_uuid(data["id"]):
        data["id"] = str(uuid.uuid4())

    # ä¿®å¤æ—¶é—´æˆ³æ ¼å¼
    if "timestamp" in data:
        try:
            dt = datetime.fromisoformat(data["timestamp"])
            data["timestamp"] = dt.isoformat() + "Z"
        except:
            data["timestamp"] = datetime.now().isoformat() + "Z"

    return data
```

### é—®é¢˜ 2: API é€Ÿç‡é™åˆ¶

**ç—‡çŠ¶**ï¼š
```
âŒ RateLimitError: Rate limit exceeded. Retry after 60 seconds.
```

**åŸå› **ï¼šçŸ­æ—¶é—´å†…å‘é€è¿‡å¤šè¯·æ±‚ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
from aiflow.adapters.base import BaseAIAdapter

# é…ç½®é‡è¯•ç­–ç•¥
adapter = await create_claude_adapter(
    max_retries=5,              # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay=10,             # åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
    exponential_backoff=True    # ä½¿ç”¨æŒ‡æ•°é€€é¿
)

# é‡è¯•é€»è¾‘
response = await adapter.generate_with_retry(prompt=prompt)
```

### é—®é¢˜ 3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
MemoryError: Unable to allocate array
```

**åŸå› **ï¼šå¤§å‹é¡¹ç›®çš„åˆ†æç»“æœè¿‡å¤§ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **ä½¿ç”¨å‹ç¼©å­˜å‚¨**
```python
serializer.serialize(
    data=result,
    output_path="analysis_result.json",
    compress=True  # å¯ç”¨ gzip å‹ç¼©ï¼ŒèŠ‚çœ 70% ç©ºé—´
)
```

2. **åˆ†é˜¶æ®µåˆ†æ**
```python
# ä¸è¦ä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰æ¨¡å—
# æŒ‰æ¨¡å—é€ä¸ªåˆ†æï¼Œé€æ­¥åˆå¹¶ç»“æœ
for module in project_modules:
    partial_result = await analyze_module(module)
    merge_into_final_result(partial_result)
```

3. **é™åˆ¶åˆ†ææ·±åº¦**
```python
# åœ¨ Prompt ä¸­é™åˆ¶æ·±åº¦
input_data = {
    "max_depth": 5,  # æœ€å¤§é€’å½’æ·±åº¦
    "max_nodes": 1000,  # æœ€å¤§èŠ‚ç‚¹æ•°
    ...
}
```

### é—®é¢˜ 4: å‰ç«¯æ¸²æŸ“å¡é¡¿

**ç—‡çŠ¶**ï¼šèŠ‚ç‚¹æ•°é‡ >1000 æ—¶ï¼ŒCytoscape.js æ¸²æŸ“ç¼“æ…¢ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **å¯ç”¨è™šæ‹ŸåŒ–æ¸²æŸ“**
```typescript
const cy = cytoscape({
  renderer: {
    hideEdgesOnViewport: true,
    hideLabelsOnViewport: true,
    textureOnViewport: true
  }
});
```

2. **åˆ†å±‚åŠ è½½**
```typescript
// åˆå§‹åªåŠ è½½é¡¶å±‚èŠ‚ç‚¹
const topLevelNodes = data.nodes.filter(n => !n.parent);
cy.add(topLevelNodes);

// æŒ‰éœ€åŠ è½½å­èŠ‚ç‚¹
cy.on('tap', 'node', (evt) => {
  const node = evt.target;
  if (node.data('hasChildren') && !node.data('childrenLoaded')) {
    loadChildren(node);
  }
});
```

3. **ä½¿ç”¨ Web Worker**
```typescript
// åœ¨ Worker ä¸­è§£æå’Œå¤„ç†æ•°æ®
const worker = new Worker('data-processor.js');
worker.postMessage({ data: largeData });
worker.onmessage = (event) => {
  renderGraph(event.data);
};
```

### é—®é¢˜ 5: AI è¾“å‡ºä¸ä¸€è‡´

**ç—‡çŠ¶**ï¼šåŒä¸€é¡¹ç›®å¤šæ¬¡åˆ†æï¼Œç»“æœå·®å¼‚å¾ˆå¤§ã€‚

**åŸå› **ï¼šAI ç”Ÿæˆå…·æœ‰éšæœºæ€§ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **é™ä½ temperature**
```python
adapter = await create_claude_adapter(
    temperature=0.1  # é™ä½éšæœºæ€§ï¼ˆé»˜è®¤ 0.7ï¼‰
)
```

2. **ä½¿ç”¨ç§å­ï¼ˆå¦‚æœ API æ”¯æŒï¼‰**
```python
response = await adapter.generate(
    prompt=prompt,
    seed=42  # å›ºå®šç§å­ç¡®ä¿å¯é‡å¤æ€§
)
```

3. **å¤šæ¬¡é‡‡æ ·å–å¹³å‡**
```python
async def consistent_analysis(prompt, num_samples=3):
    """å¤šæ¬¡é‡‡æ ·ï¼Œå–æœ€ä¸€è‡´çš„ç»“æœ"""
    results = []
    for _ in range(num_samples):
        result = await adapter.generate(prompt=prompt)
        results.append(result)

    # é€‰æ‹©æœ€ä¸€è‡´çš„ç»“æœï¼ˆä¾‹å¦‚ï¼Œè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
    return select_most_consistent(results)
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. Prompt ä¼˜åŒ–æŠ€å·§

**DO âœ…**ï¼š
- æä¾›å®Œæ•´çš„è¾“å…¥è¾“å‡ºç¤ºä¾‹
- ä½¿ç”¨ç»“æ„åŒ–æ ‡ç­¾ï¼ˆ`<thinking>`, `<output>`ï¼‰
- æ˜ç¡®æ‰€æœ‰çº¦æŸæ¡ä»¶ï¼ˆUUIDæ ¼å¼ã€æ—¶é—´æ ¼å¼ï¼‰
- è¦æ±‚ AI æä¾›ç½®ä¿¡åº¦å’Œæ¨ç†ä¾æ®

**DON'T âŒ**ï¼š
- ä½¿ç”¨æ¨¡ç³Šçš„æŒ‡ä»¤ï¼ˆ"åˆ†æä¸€ä¸‹"ï¼‰
- å‡è®¾ AI çŸ¥é“é»˜è®¤æ ¼å¼
- å¿½ç•¥è¾¹ç•Œæƒ…å†µ
- ä¸éªŒè¯ AI è¾“å‡º

### 2. æ€§èƒ½ä¼˜åŒ–å»ºè®®

**ç¼“å­˜ç­–ç•¥**ï¼š
```python
# ç¼“å­˜ Prompt æ¨¡æ¿
template_cache = {}

def load_template_cached(language, stage):
    key = f"{language}:{stage}"
    if key not in template_cache:
        template_cache[key] = load_template(language, stage)
    return template_cache[key]
```

**å¹¶å‘æ§åˆ¶**ï¼š
```python
# æ§åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
from asyncio import Semaphore

semaphore = Semaphore(5)  # æœ€å¤š 5 ä¸ªå¹¶å‘è¯·æ±‚

async def analyze_with_limit(prompt):
    async with semaphore:
        return await adapter.generate(prompt=prompt)
```

### 3. æ•°æ®ç®¡ç†å»ºè®®

**ç‰ˆæœ¬æ§åˆ¶**ï¼š
```bash
# ä¸ºæ¯æ¬¡åˆ†æåˆ›å»ºç‰ˆæœ¬å·
analysis_result_v1.0.0.json
analysis_result_v1.1.0.json

# ä½¿ç”¨ Git ç®¡ç†åˆ†æç»“æœ
git add analysis_result*.json
git commit -m "chore: update code analysis"
```

**å¤‡ä»½ç­–ç•¥**ï¼š
```python
import shutil
from datetime import datetime

# è‡ªåŠ¨å¤‡ä»½æ—§ç»“æœ
if os.path.exists("analysis_result.json"):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"backup/analysis_result_{timestamp}.json"
    shutil.copy("analysis_result.json", backup_file)
```

### 4. å›¢é˜Ÿåä½œå»ºè®®

**åˆ†äº«æœ€ä½³å®è·µ**ï¼š
- å°†è‡ªå®šä¹‰ Prompt æ¨¡æ¿æäº¤åˆ°å›¢é˜Ÿä»“åº“
- æ–‡æ¡£åŒ–ç‰¹å®šé¡¹ç›®çš„åˆ†æé…ç½®
- åˆ†äº«åˆ†æç»“æœçš„è§£è¯»æŠ€å·§

**Code Review é›†æˆ**ï¼š
```bash
# åœ¨ PR ä¸­è‡ªåŠ¨ç”Ÿæˆä»£ç åˆ†æ
name: Code Analysis

on: pull_request

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run AIFlow Analysis
        run: python scripts/analyze.py
      - name: Upload Result
        uses: actions/upload-artifact@v3
        with:
          name: analysis-result
          path: analysis_result.json
```

---

## ğŸ“š æ‰©å±•é˜…è¯»

- [æ¶æ„è®¾è®¡æ–‡æ¡£](./ARCHITECTURE.md) - æ·±å…¥ç†è§£ç³»ç»Ÿè®¾è®¡
- [Prompt è®¾è®¡æ–‡æ¡£](./PROMPT_DESIGN.md) - å­¦ä¹ å¦‚ä½•ä¼˜åŒ–æŒ‡ä»¤é›†
- [æ ‡å‡†æ•°æ®åè®®](../backend/aiflow/protocol/schemas/analysis-schema-v1.0.0.json) - å®Œæ•´çš„ JSON Schema

---

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿè¯•è¯•è¿™äº›èµ„æºï¼š

1. **GitHub Issues**: [æäº¤é—®é¢˜](https://github.com/yourusername/aiflow/issues)
2. **Discussions**: [ç¤¾åŒºè®¨è®º](https://github.com/yourusername/aiflow/discussions)
3. **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£](https://docs.aiflow.dev)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-01-12
**ç»´æŠ¤è€…**: AIFlow Team
